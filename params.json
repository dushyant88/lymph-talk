{"name":"Lymph Talk","tagline":"An introduction talk about http://lymph.io","body":"# Stop trying to glue your services together`; import lymph`\r\n\r\nWelcome to the materials and the transcript of our talk about lymph.\r\n\r\n## Demo setup\r\n\r\nIt is suggest to use the provided [vagrant](vagrantup.com) box. It is set up\r\nwith all tooling and code ready for your perusal. It has both zookeeper and\r\nrabbitmq running inside. To get it up and running execute:\r\n\r\n``` shell\r\nvagrant up && vagrant ssh\r\n```\r\n\r\nYou will be prompted for the root password half-way through `vagrant up`\r\nbecause we use NFS to share files.\r\n\r\nOnce inside, the `motd` contains all information you need to get going.\r\n\r\n## The talk\r\n> An introduction talk about lymph by Alejandro Castillo & Max Brauer\r\n\r\n(This is a transcript. So it might read odd here and there.)\r\n\r\n### Opening\r\n\r\nHello and good afternoon. Hopefully you've had a nice lunch. My name is\r\n\\<name\\> and I'd like to introduce you to _lymph_, a framework for writing\r\nservices in Python. With lymph you can write services with almost no\r\nboilerplate. But let me introduce us first.\r\n\r\nWe're Delivery Hero, a holding of online food ordering services world-wide.\r\nWe're located in Berlin. We operate in 34 countries and growing.\r\n\r\nLet me explain the concept of online food ordering to those who're unfamiliar\r\nwith it. However, I doubt there arent any ;) The concept is simple:\r\n* get hungry\r\n* go online\r\n* search for restaurants close to you\r\n* compile your order\r\n* pay online\r\n* wait for the delivery\r\nBasically, it's e-commerce with very grumpy customers. But the restaurant\r\nintegration, e.g. order transmission, fulfillment, delivery, etc. offer quite\r\nan ecosystem of things to tackle.\r\n\r\nLet's briefly go over the flow of our talk:\r\n* We're going to explain where we're coming from and why we have given birth\r\n  to another framework\r\n* We'll look at code as fast possible\r\n* We'll run services and increasingly add new services to explore communication\r\n  patterns and characteristics of lymph\r\n* We'll give you a brief rundown of lymph's internal\r\n* We'll go over how lymph is different from nameko\r\n* We'll talk future plans\r\n\r\nFor the sake of this talk we assume that you're familiar with the concept of\r\nservices. We assume that you are familiar with what a monolith is. We assume\r\nthat you are familiar with when and why to use either and even more so when\r\nnot. We will not discuss the differences between monoliths and services. We\r\nwon't talk about how services might safe your development or your business.\r\nNeither will talk about sophisticated networking topologies on how to plug\r\nservices together.\r\n\r\nBut what we're going to talk about is lymph. By the end of the talk you should\r\nunderstand what lymph can and cannot do and why that's cool.\r\n\r\nYou can find all sources and this talk written down online. I'll share it with\r\nyou afterwards, otherwise my appearance here is pointless ^^\r\n\r\nIf you're Spanish speaker you may also want to attend this very talk in Spanish\r\nlater this weak by Castillo.\r\n\r\nIf you happen to attend the PyCon France, we'll meet there too.\r\n\r\nYou can find us in the foyer. Talk to us :) We've brought goodies and gummi\r\nbears.\r\n\r\n(repeat all this in the end)\r\n\r\n### Motivation\r\n\r\nOur starting point was the classic situation. We had a massive Django monolith.\r\nWe weren't moving fast at all. We've had trouble finding rhythm for a growing\r\nnumber of teams and developers. People we're blocked by other people. You\r\nshould think textbook \"my monolith hurts, i want services\". So, the perks of a\r\nmore service-oriented became increasingly attractive and reasonable to us. This\r\nwas even more so in the light of a global platform to unite our very\r\nheterogenic Product landscape. #modularity\r\n\r\nSo, the first thing some of you would be thinking is 'why write another\r\nframework?'. The answer was that when we looked around we did not find\r\nanything that would fit our needs. We wanted to work with services but we\r\nwanted some very specific things(@TODO what are these?). We are mainly\r\nPython-powered so we wanted to stay inside Python as much as possible. We\r\nwanted to abstract away all the problems one is dealing with when doind\r\nservices, i.e. transport data, register services, discover them etc. We wanted\r\nto enable our developers to work with services in a simple and easy way: no\r\nboilerplate, no excessive details that don't relate to bussiness logic.\r\n\r\n@TODO: prepare other alternatives and related technologies and possible go over\r\nthem (jsonrpc, zerorpc, chaussette, cocaine, ...)\r\n\r\nBy the way, how many of you have attended the nice talk about\r\n[nameko](https://github.com/onefinestay/nameko)?\r\n\r\nYes, some of you will be thinking after their nice talk 'use Nameko'. But that\r\nwas not in the current state when we started. It also does not cover all the\r\nthings we wanted to get from a framework. Nevermind though, we'll briefly go\r\nover the differences of the two later.\r\n\r\nSo, say hello to [lymph][lymph.io]. By now, hopefully, you're itching to\r\nsee how a service looks in lymph. Spoiler alert: very much like in nameko.\r\n\r\nWe'll break the ice by demoing running and playing around with services. We'll\r\nslowly progress through lymph's features, service by service.\r\n\r\n### Demo\r\n\r\n#### The Echo service\r\n\r\n[show Echo service code]\r\n\r\nEt voila. This is what a simple echo service looks like in lymph. Its interface\r\nis one RPC method called `echo` which takes text, prints it, emits an\r\nevent(containing the text in the body) and returns the text.\r\n\r\nAll we need to do to make things happen is to inherit from `lymph.Interface`\r\nand decorate RPC methods with `@lymph.rpc()`. Lastly, we've got the interface's\r\n`emit()` function to our disposal which dispatches events in the event system.\r\n\r\nLet's jump on the shell and play with it.\r\n\r\n``` shell\r\nmux start echo\r\n```\r\n\r\nWhat you see here is a tmux session with two panes. On the right-hand side you\r\nsee the echo service being run with lymph's `instance` command. On the\r\nleft-hand side you see a plain shell on which we'll explore lymph's tooling.\r\n\r\nOne of the first things we considered when building lymph was the tooling. We\r\nthink we managed to get some very nice tooling built around it to make\r\ndevelopment of services easier.\r\n\r\nSo what tooling is available? `lymph list` will tell us.\r\n\r\n``` shell\r\nlymph list\r\n```\r\n\r\nYou see there's plenty of commands available to interact with services.\r\n\r\nTo begin with let's assert that an instance of the echo service is running.\r\nWe'll use lymph's `discover` command.\r\n\r\n``` shell\r\nlymph discover\r\n```\r\n\r\nAs you can see, one instance is running indeed (`Echo [1]`).\r\n\r\nLet's excercise the echo service's `echo` method. We'll use lymph's `request`\r\ncommand. Therefore, we have to provide the service name, the name of the method\r\nand the body of the request as JSON. What we expect to see is the echo service\r\nto return the text as is, but it should also print it and emit an event.\r\n\r\n``` shell\r\nlymph request Echo.echo '{\"text\": \"Good afternoon, EuroPython!\"}'\r\n```\r\n\r\nThe result of the RPC is as expected and the service printed the text.\r\n\r\nThis is boring and our service must be pretty lonely. Nobody listens to its\r\nevents. Here comes the ear.\r\n\r\n#### The Ear service\r\n\r\n[show Ear service code]\r\n\r\nThe ear listens to echo's events. Pardon the pun. Again, it's a lymph\r\nservice(we inherit from `lymph.Interface`). However, there's nothing but one\r\nmethod which is subscribed to `echo` events. It simply prints the text contained\r\nin the event's body. That means, everytime an event of this type occurs\r\nexactly once instance of ear will consume it.\r\n\r\nLet's excercise our services combination. This time round, though, we'll run\r\ntwo instances of the echo service and one instance of the ear service.\r\n\r\n``` shell\r\nmux start echo-ear\r\n```\r\n\r\nAgain, we see a tmux session. On the right you find two instances of the echo\r\nservice followed by an instance of the ear service.\r\n\r\nWe should find them registered correctly.\r\n\r\n``` shell\r\nlymph discover\r\n```\r\n\r\nAnd, indeed, they list correctly.\r\n\r\nLet's emit an echo event in the event system to assert whether the ear service\r\nlistens to it. We'll use lymph's `emit` command. We're expecting the ear to\r\nprint the text field from the event body.\r\n\r\n```\r\nlymph emit echo '{\"text\": \"hi\"}'\r\n```\r\n\r\nNice. That worked.\r\n\r\nWhen we do RPCs now we expect the echo instances to respond in round-robin\r\nfashion. Furthermore, the ear instance should print all consumed events.\r\n\r\n``` shell\r\nlymph request Echo.echo '{\"text\": \"Good afternoon, EuroPython!\"}'\r\n```\r\n(repeatedly, until both echo instances have responded)\r\n\r\nAs you see, our expectations are met.\r\n\r\nIf we were to run several instances of the ear services, each event would be\r\nconsumed by exactly once instance. However, lymph allows to broadcast events.\r\n\r\nFinally, since it's 2015, no talk would be complete without talking about HTTP.\r\nLet's add a web service to the mix. Let's say we wanted to expose the echo\r\nfunctionality via an HTTP API. Lymph has a class for that.\r\n\r\n#### The Web service\r\n\r\n[show Web service code]\r\n\r\nThis is the Web service. It subclasses lymph's `WebServiceInterface`. In this\r\ncase we're not exposing RPC methods, emitting not listening to events. However,\r\nwe configure a Werkzeug URL map as a class attribute. We've added one endpoint:\r\n`/echo` and a handler for it. The handler receives a Werkzeug reuqest object.\r\n\r\nThe echo handler unpacks the body of the request. It calls the echo service\r\nvia the `self.proxy` and returns the result in the response. And it prints.\r\n\r\nMind, that we're not validating the request method nor anything else.\r\n\r\nRun it or it didn't happen. We'll bring up an instance of each of services now.\r\n\r\n``` shell\r\nmux start all\r\n```\r\n\r\nOn the right you can see an instance of every service, Web, Echo and Ear.\r\n\r\nOnce again, they should have registered correctly:\r\n\r\n``` shell\r\nlymph discover\r\n```\r\n\r\nLet's hit our web service and see how the request ripples through our service\r\ncluster. We should see all service print something. The web service is\r\nlistening at the default port 4080. We're using `httpie` to excercise the\r\nrequest:\r\n\r\n```\r\nhttp localhost:4080/echo text=hi\r\n```\r\n\r\nThe response looks good and all services have performed accordingly.\r\n\r\n#### Lymph's development server\r\n\r\nYet, when developing locally you seldomly would want to run all of your\r\nservices within different shell or tmux panes. Lymph has its own development\r\nserver which wraps around any number of services with any number of instances\r\neach. Therefore, we'll have to configure which services to run and how many\r\ninstances of each in the `.lymph.yml`:\r\n\r\n``` yaml\r\ninstances:\r\n    Web:\r\n        command: lymph instance --config=conf/web.yml\r\n\r\n    Echo:\r\n        command: lymph instance --config=conf/echo.yml\r\n        numprocesses: 3\r\n\r\n    Ear:\r\n        command: lymph instance --config=conf/ear.yml\r\n        numprocesses: 2\r\n```\r\n\r\nMind that in our case `command` specifies lymph instances but this could also\r\nbe any other service you need, e.g. Redis.\r\n\r\nLet's run it.\r\n\r\n``` shell\r\nmux start all\r\n```\r\n\r\nOnce more, we find ourselves inside a tmux session with lymph node running in\r\nthe top-right pane. Below that you see lymph tail running which allows us to\r\nfollow the logs of any number of services. But first, let's check how many instances\r\nare running:\r\n\r\n``` shell\r\nlymph discover\r\n```\r\n\r\nThat's a good number. Once we feed a request into the cluster we should see print\r\nstatements and logs appearing.\r\n\r\n``` shell\r\nhttp localhost:4080/echo text=hi\r\n```\r\n\r\nBut there's a lot going on. You would find an even bigger mess the more\r\nservices and instances you run and the more intricated your patterns of\r\ncommunication become. Sometimes you wonder \"where did my request go?\". Lymph\r\nhelps you though with `trace\\_id`s. Every request that appears in our cluster\r\nwhich doesn't have a trace_id assigned gets one. These trace_ids get fowarded\r\nvia every RPC and event.\r\n\r\nSo we should be able to corellate all actions in cluster to the one incoming\r\nHTTP request.\r\n\r\n[use iterms highlighting: Ctrl+f and type 'trace_id']\r\n\r\nAnd indeed we see the same trace_id across our service instances.\r\n\r\nAnd that mostly covers the tooling we have for lymph services. Let's talk about\r\nlymph's stack next.\r\n\r\n#### Things we haven't touched\r\n* lymph subscribe\r\n* lymph shell\r\n* config API\r\n* on_start\r\n* metrics\r\n* plugins (new relic, sentry, lymph-top)\r\n\r\n### Lymph under the hood\r\n\r\nHow does lymph do things under the hood?\r\n\r\n* greenlets\r\n* rpc via 0mq\r\n* events via rabbitmq (pluggable)\r\n* registry via zk (pluggable)\r\n* http with werkzeug\r\n* testing\r\n\r\n### Lymph compared to Nameko\r\n* (http://lucumr.pocoo.org/2015/4/8/microservices-with-nameko/)\r\n* tech\r\n* running\r\n* testing\r\n* only rabbitmq, no zk\r\n\r\n### Future\r\n* eco system (storage, storeproxy, flow etc)\r\n* distconfig\r\n\r\n### Summary & outro\r\n* did we accomplish? circle back to claim and title\r\n* lymph.io\r\n* we accept PRs\r\n* we're hiring\r\n* same talk in Spanish\r\n* PyCon Fr\r\n\r\n### Q&A\r\n* why zookeeper for registry?\r\n* how to scale up web services? (sharing sockets)\r\n \r\n### Nice to have\r\n* plugins (lymph-top, newrelic, sentry)\r\n* monitoring\r\n* serial events & broadcast(websockets)\r\n* sieve of Erathostenes (Mislav)\r\n","google":"UA-64656787-1","note":"Don't delete this file! It's used internally to help with page regeneration."}